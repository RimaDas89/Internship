{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cefaa5",
   "metadata": {},
   "source": [
    "# BANK MARKETING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6d10d",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score,recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a63f",
   "metadata": {},
   "source": [
    "## 2. Importing Data & Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4371d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:\\\\Users\\\\Rima Das\\\\OneDrive\\\\Desktop\\\\Third-Phase Practice project\\\\BANK MARKETING\\\\termdeposit_train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf891f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv(\"C:\\\\Users\\\\Rima Das\\\\OneDrive\\\\Desktop\\\\Third-Phase Practice project\\\\BANK MARKETING\\\\termdeposit_test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Dimension of Train & Test data\n",
    "\n",
    "print(\"In Train dataset we have {} rows & {} columns\".format(*train.shape))\n",
    "print(\"In Test dataset we have {} rows & {} columns\".format(*test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90882a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Names in Train & Test Dataset\n",
    "\n",
    "print(\"These are the columns present in TRAIN dataset: \\n\",train.columns)\n",
    "print(\"\\nThese are the columns present in TEST dataset: \\n\",test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed567a",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "* **Train DataFrame:**\n",
    "    * In Train dataset we can see that we have 31647 rows & 18 columns.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "* **Test DataFrame:**\n",
    "    * In test Dataset we can see that we have 13564 rows & 17 columns.\n",
    "    * Here 1 column is missing because thats our target/label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data information\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb300c",
   "metadata": {},
   "source": [
    "### `Observations`\n",
    "* **Train Data**\n",
    "    * We can see there are 18 columns including out Target column\n",
    "    * Among 18 column, we have 8 columns contains integar values & 10 columns has object values\n",
    "    * Our target column has object values\n",
    "    * Memory Usage 4.3+ MB\n",
    "    \n",
    "&nbsp;\n",
    "\n",
    "* **Test Data**\n",
    "    * There are 17 columns no target column is there\n",
    "    * out of 17 columns, 8 columns has integar values and 9 columns has object values.\n",
    "    * Memory Usage: 1.8+ MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f61d68",
   "metadata": {},
   "source": [
    "## 3. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b38fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing it\n",
    "sns.heatmap(train.isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7c954",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "* There are no null values in our datasets(train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd11354",
   "metadata": {},
   "source": [
    "## 4. Duplicate Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have {} duplicated values in our TRAIN datasets'.format(train.duplicated().sum()))\n",
    "print('We have {} duplicated values in our TEST datasets'.format(test.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98347b6d",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "* We have no duplicated values in both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe46a37",
   "metadata": {},
   "source": [
    "## 5. Separating Categorical and numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaparating Categrocial columns in train Dataframe\n",
    "cat_col = train.select_dtypes(include='object').columns\n",
    "print(\"Categorial columns in our train dataframe:\\n\",cat_col)\n",
    "\n",
    "#Seaparating Numerical columns in train Dataframe\n",
    "num_col = train.select_dtypes(include='int64').columns\n",
    "print(\"\\nNumerical columns in our train dataframe:\\n\",num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22000631",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical summary of Numerical colums\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a3a89",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "1. **ID**: \n",
    "   - The 'ID' column appears to represent unique identifier values.\n",
    "   - The range of 'ID' values starts from 2 and goes up to 45,211, suggesting that there are a total of 31,647 unique records in the dataset.\n",
    "   \n",
    "2. **age**: \n",
    "   - The 'age' column represents the age of individuals.\n",
    "   - The mean age in the dataset is approximately 40.96 years, with a minimum age of 18 years and a maximum age of 95 years.\n",
    "   - The majority of individuals (at least 25%) fall in the age range of 33 to 48 years.\n",
    "   \n",
    "3. **balance**: \n",
    "   - The 'balance' column likely represents the financial balance of individuals.\n",
    "   - The mean balance is approximately 1363.89 units, with a wide range of values.\n",
    "   - The distribution of 'balance' appears to be positively skewed, as the maximum value is significantly higher than the mean, indicating the presence of outliers.\n",
    "\n",
    "4. **day**: \n",
    "   - The 'day' column might represent the day of the month when the data was recorded.\n",
    "   - The values in this column range from 1 to 31, suggesting that it could be associated with calendar days.\n",
    "\n",
    "5. **duration**: \n",
    "   - The 'duration' column may represent the duration of a contact or interaction.\n",
    "   - The mean duration is approximately 258.11 seconds (or about 4.3 minutes).\n",
    "   - The maximum duration is considerably higher at 4918 seconds, indicating the presence of long-duration interactions.\n",
    "\n",
    "6. **campaign**: \n",
    "   - The 'campaign' column likely represents the number of contacts made during a marketing campaign.\n",
    "   - On average, individuals were contacted approximately 2.77 times during a campaign.\n",
    "   - The maximum number of campaign contacts for an individual is 63, suggesting some extreme values.\n",
    "\n",
    "7. **pdays**: \n",
    "   - The 'pdays' column may indicate the number of days since the last contact.\n",
    "   - The majority of values in this column appear to be -1, which might represent that the client was not previously contacted.\n",
    "   - The maximum value is 871, suggesting a significant time gap for some clients between contacts.\n",
    "\n",
    "8. **previous**: \n",
    "   - The 'previous' column may represent the number of previous marketing contacts.\n",
    "   - On average, individuals have had approximately 0.57 previous contacts.\n",
    "   - The maximum number of previous contacts is 275, indicating that some individuals have been contacted multiple times in previous campaigns.\n",
    "\n",
    "These observations provide an initial understanding of the TRAIN dataset's numerical features, including central tendencies, spreads, and potential outliers. Further data analysis and preprocessing may be required to gain deeper insights and prepare the data for modeling or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe7e85",
   "metadata": {},
   "source": [
    "## 7. Data Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d0745",
   "metadata": {},
   "source": [
    "### Exploring Target/ Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f656d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts for 'subscribed' column\n",
    "value_counts = train['subscribed'].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = (value_counts / len(train)) * 100\n",
    "\n",
    "# Combine value counts and percentages into a DataFrame\n",
    "result_df = pd.DataFrame({'Count': value_counts, 'Percentage': percentages})\n",
    "\n",
    "# Print the result DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value Count of Subscribed column\n",
    "sns.countplot(x='subscribed', data=train, ax=axes)\n",
    "axes.set_title('Value Count of Subscribed column')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf19b15",
   "metadata": {},
   "source": [
    "### `Observation:`\n",
    "\n",
    "- The 'subscribed' column has two unique values: 'no' and 'yes'.\n",
    "- The total count of 'no' in the column is 27,932.\n",
    "- The total count of 'yes' in the column is 3,715.\n",
    "- The percentage of 'no' in the column is approximately 88.26%.\n",
    "- The percentage of 'yes' in the column is approximately 11.74%.\n",
    "\n",
    "This indicates that the majority of the data falls under the 'no' category in the 'subscribed' column, with 'yes' accounting for a smaller portion of the data. Thats means our data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74f93b",
   "metadata": {},
   "source": [
    "## a. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf8ec9",
   "metadata": {},
   "source": [
    "### i. Categorical columns of train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a026ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking Value Counts of Categorical Column(Train Data)\n",
    "cat_col = cat_col[:-1]\n",
    "\n",
    "for col in cat_col:\n",
    "    print('*'*10,col,'*'*10)\n",
    "    print(train[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f51d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualizing it\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(cat_col), 1, figsize=(6, 4*len(cat_col)))\n",
    "\n",
    "# Iterate through categorical columns\n",
    "for i, column in enumerate(cat_col):\n",
    "    # Count plot\n",
    "    sns.countplot(x=column, data=train, ax=axes[i])\n",
    "    axes[i].set_title(f\"Value count of {column}\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbac89e",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "**job**:\n",
    "- The 'job' column represents the occupation or job type of individuals.\n",
    "- The most common job categories are 'blue-collar,' 'management,' 'technician,' and 'admin.'\n",
    "- 'Blue-collar' is the most frequent job category, with 6,842 individuals having this occupation.\n",
    "\n",
    "**marital**:\n",
    "- The 'marital' column represents the marital status of individuals.\n",
    "- The most common marital status categories are 'married' and 'single.'\n",
    "- 'Married' is the most frequent marital status, with 19,095 individuals.\n",
    "\n",
    "**education**:\n",
    "- The 'education' column represents the education level of individuals.\n",
    "- The majority of individuals have 'secondary' education, followed by 'tertiary' and 'primary' education.\n",
    "- 'Secondary' education is the most common, with 16,224 individuals having this education level.\n",
    "\n",
    "**default**:\n",
    "- The 'default' column likely indicates whether individuals have credit in default.\n",
    "- The majority of individuals do not have credit in default ('no'), while a smaller proportion have credit in default ('yes').\n",
    "\n",
    "**housing**:\n",
    "- The 'housing' column may indicate whether individuals have housing loans.\n",
    "- More individuals have housing loans ('yes') compared to those without ('no').\n",
    "\n",
    "**loan**:\n",
    "- The 'loan' column represents whether individuals have personal loans.\n",
    "- Most individuals do not have personal loans ('no').\n",
    "\n",
    "**contact**:\n",
    "- The 'contact' column may indicate the method of communication.\n",
    "- 'Cellular' is the most common contact method, followed by 'unknown' and 'telephone.'\n",
    "\n",
    "**month**:\n",
    "- The 'month' column represents the month in which contacts were made.\n",
    "- The most frequently used contact month is 'May,' followed by 'July' and 'August.'\n",
    "\n",
    "**poutcome**:\n",
    "- The 'poutcome' column might represent the outcome of a previous marketing campaign.\n",
    "- The most common outcome is 'unknown,' followed by 'failure,' 'other,' and 'success.'\n",
    "\n",
    "These observations provide insights into the distribution and frequencies of categories within each categorical variable, which is valuable for understanding the characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063378e",
   "metadata": {},
   "source": [
    "### ii. Numerical Column of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22155ebb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Creating for loop to iterate over Numerical columns\n",
    "\n",
    "for col in num_col:\n",
    "    print('*'*10,col,'*'*10)\n",
    "    print(train[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing numerical columns\n",
    "plt.figure(figsize=(20, 15), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in num_col:\n",
    "    if plotnumber <= 8:\n",
    "        ax = plt.subplot(4,2, plotnumber)\n",
    "        sns.histplot(train[column], color='g')\n",
    "        plt.xlabel(column, fontsize=16)\n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522bdd2a",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "**ID:**\n",
    "- The ID column appears to be unique, with each value occurring only once. This suggests that it might be an identifier for each row.\n",
    "\n",
    "**Age:**\n",
    "- The age column shows a distribution of ages, with the most common ages being around 32, 31, and 33.\n",
    "- Age varies from 18 to 95.\n",
    "\n",
    "**Balance:**\n",
    "- The balance column has a wide range of values, including both positive and negative balances.\n",
    "- The most common balance value is 0, followed by 1 and 2.\n",
    "\n",
    "**Day:**\n",
    "- The day column represents the day of the month. It shows a distribution of days, with some days being more common than others.\n",
    "- The most common days are around the 20th, 18th, and 21st of the month.\n",
    "\n",
    "**Duration:**\n",
    "- The duration column represents the duration of the contact in seconds. It varies widely, with many different values.\n",
    "- There are multiple occurrences of similar durations, indicating that certain call durations are more common.\n",
    "\n",
    "**Campaign:**\n",
    "- The campaign column represents the number of contacts performed during the campaign. It shows a distribution with most values between 1 and 10 contacts.\n",
    "- There are a few outliers with higher numbers of contacts.\n",
    "\n",
    "**Pdays:**\n",
    "- The pdays column represents the number of days since the client was last contacted. The majority of values are -1, indicating that most clients were not previously contacted.\n",
    "- There are a few positive values indicating the number of days since the last contact for some clients.\n",
    "\n",
    "**Previous:**\n",
    "- The previous column represents the number of contacts performed before this campaign. It shows a distribution with most values being 0.\n",
    "- There are some non-zero values, indicating that some clients were contacted in previous campaigns.\n",
    "\n",
    "These observations provide insights into the distribution and characteristics of the numerical columns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bce042",
   "metadata": {},
   "source": [
    "## b. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00609f4",
   "metadata": {},
   "source": [
    "#### i. Categorical Columns Vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table for 'job' and 'subscribed'\n",
    "job_crosstab = pd.crosstab(train['job'], train['subscribed'])\n",
    "\n",
    "# Calculate the percentage\n",
    "job_crosstab_norm = job_crosstab.div(job_crosstab.sum(1).astype(float), axis=0)\n",
    "\n",
    "# Concatenate the contingency table and normalized contingency table side by side\n",
    "concatenated_tables = pd.concat([job_crosstab, job_crosstab_norm], axis=1, keys=['Count', 'Percentage'])\n",
    "concatenated_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4130f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 4))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value count of Subscribed with Job Role\n",
    "sns.countplot(x='job', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"}, ax=axes)\n",
    "axes.set_title('Value count of Subscribed with Job Role', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7697f",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "1. **Job vs. Subscription Count:**\n",
    "   - Among the different job categories, \"retired\" and \"student\" have the highest counts of \"yes\" subscriptions, with 362 and 182 respectively.\n",
    "   - \"Entrepreneur\" and \"unknown\" have the lowest counts of \"yes\" subscriptions, with 85 and 26 respectively.\n",
    "   - \"Admin\" and \"blue-collar\" jobs have relatively high counts of both \"yes\" and \"no\" subscriptions, with 452 and 489 \"yes\" subscriptions, and 3179 and 6353 \"no\" subscriptions respectively.\n",
    "\n",
    "2. **Job vs. Subscription Percentage:**\n",
    "   - When looking at the percentage of \"yes\" subscriptions within each job category, \"student\" stands out with the highest percentage at approximately 28.66%.\n",
    "   - \"Retired\" individuals also have a significant percentage of \"yes\" subscriptions at around 23.00%.\n",
    "   - \"Blue-collar\" workers have the lowest percentage of \"yes\" subscriptions, accounting for only about 7.15% of the total in their category.\n",
    "   - \"Management\" professionals have a relatively high percentage of \"yes\" subscriptions, with approximately 13.90%.\n",
    "\n",
    "3. **Overall:**\n",
    "   - The percentage of \"yes\" subscriptions varies significantly across different job categories, indicating that job type may be a significant factor in predicting whether an individual subscribes to the service or not.\n",
    "   - \"Student\" and \"retired\" individuals are more likely to subscribe, while \"blue-collar\" workers are less likely to subscribe.\n",
    "\n",
    "These observations provide insights into how the \"job\" variable relates to the subscription outcome, which can be valuable for targeted marketing strategies or further analysis in the context of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marital status vs subscribed\n",
    "marital_crosstab= pd.crosstab(train['marital'], train['subscribed'])\n",
    "\n",
    "#Calculating the percentage\n",
    "marital_crosstab_norm = marital_crosstab.div(marital_crosstab.sum(1).astype(float), axis=0)\n",
    "\n",
    "#concatenate marital crosstab and marital crosstab norm side by side\n",
    "marital_concat=pd.concat([marital_crosstab, marital_crosstab_norm], axis=1, keys=[\"count\", \"Percentage\"])\n",
    "marital_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695347cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 4))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value count of Subscribed with Marital Status\n",
    "sns.countplot(x='marital', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"}, ax=axes)\n",
    "axes.set_title('Value count of Subscribed with Marital Status', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb8313",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "1. **Marital Status vs. Subscription Count:**\n",
    "   - Among the different marital statuses, individuals who are \"married\" have the highest count of \"no\" subscriptions, with 17,176 individuals.\n",
    "   - On the other hand, individuals who are \"single\" have the highest count of \"yes\" subscriptions, with 1,351 individuals.\n",
    "   - \"Divorced\" individuals fall in between, with 3,185 \"no\" subscriptions and 445 \"yes\" subscriptions.\n",
    "\n",
    "2. **Marital Status vs. Subscription Percentage:**\n",
    "   - When looking at the percentage of \"yes\" subscriptions within each marital status category:\n",
    "     - \"Single\" individuals have the highest percentage of \"yes\" subscriptions at approximately 15.14%.\n",
    "     - \"Divorced\" individuals have the lowest percentage of \"yes\" subscriptions, accounting for about 12.26% of their category.\n",
    "     - \"Married\" individuals have a moderate percentage of \"yes\" subscriptions, approximately 10.05%.\n",
    "\n",
    "3. **Overall:**\n",
    "   - The marital status appears to have an influence on the subscription outcome.\n",
    "   - \"Single\" individuals have a notably higher likelihood of subscribing compared to \"married\" and \"divorced\" individuals.\n",
    "   - These observations suggest that marital status may be a relevant factor in predicting whether an individual subscribes to the service or not.\n",
    "\n",
    "These insights into the relationship between marital status and subscription status can be valuable for marketing strategies and further analysis in the context of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education Vs subscribed\n",
    "\n",
    "edu_crosstab =pd.crosstab(train['education'],train['subscribed'])\n",
    "\n",
    "# Calculate the percentage\n",
    "edu_crosstab_norm = edu_crosstab.div(edu_crosstab.sum(1).astype(float),axis=0)\n",
    "\n",
    "# Concatenated the contingency table and normalize contingency table side by side\n",
    "edu_concat = pd.concat([edu_crosstab, edu_crosstab_norm], axis=1, keys=['Count','Percentage'])\n",
    "edu_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90554aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "\n",
    "# Count plot: Value count of Subscribed with Marital Status\n",
    "sns.countplot(x='marital', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"})\n",
    "plt.title('Value count of Subscribed with Marital Status', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287f3e7",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "* **Primary Education:**\n",
    "        - Count of \"No\" Subscriptions: 4,381\n",
    "        - Count of \"Yes\" Subscriptions: 427\n",
    "        - Percentage of \"No\" Subscriptions: 91.12%\n",
    "        - Percentage of \"Yes\" Subscriptions: 8.88%\n",
    "    \n",
    "* **Secondary Education:**\n",
    "        - Count of \"No\" Subscriptions: 14,527\n",
    "        - Count of \"Yes\" Subscriptions: 1,697\n",
    "        - Percentage of \"No\" Subscriptions: 89.54%\n",
    "        - Percentage of \"Yes\" Subscriptions: 10.46%\n",
    "    \n",
    "* **Tertiary Education:**\n",
    "        - Count of \"No\" Subscriptions: 7,886\n",
    "        - Count of \"Yes\" Subscriptions: 1,415\n",
    "        - Percentage of \"No\" Subscriptions: 84.79%\n",
    "        - Percentage of \"Yes\" Subscriptions: 15.21%\n",
    "    \n",
    "* **Unknown Education:**\n",
    "        - Count of \"No\" Subscriptions: 1,138\n",
    "        - Count of \"Yes\" Subscriptions: 176\n",
    "        - Percentage of \"No\" Subscriptions: 86.61%\n",
    "        - Percentage of \"Yes\" Subscriptions: 13.39%\n",
    "\n",
    "These observations provide insights into the distribution of subscriptions based on education levels. It appears that the percentage of \"Yes\" subscriptions is higher among individuals with tertiary education compared to other education categories, while the primary education category has the highest percentage of \"No\" subscriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2eaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default vs Subscribed\n",
    "\n",
    "default_crosstab=pd.crosstab(train['default'],train['subscribed'])\n",
    "\n",
    "#Percentage Calculate\n",
    "default_crosstab_norm = default_crosstab.div(default_crosstab.sum(1).astype(float),axis=0)\n",
    "\n",
    "#concate both tables\n",
    "default_concat=pd.concat([default_crosstab, default_crosstab_norm], axis=1, keys=['Count', 'Percentage'])\n",
    "default_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2108bf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing it: Value count of Default vs Subscribed\n",
    "sns.countplot(x='default', hue='subscribed', data=train)\n",
    "plt.title('Value count of Subscribed with Default', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66568c8d",
   "metadata": {},
   "source": [
    "### `Observation`:\n",
    "\n",
    "- For customers with no previous default (default=no):\n",
    "  - Count: 27,388 customers did not subscribe ('no'), and 3,674 customers subscribed ('yes').\n",
    "  - Percentage: Approximately 88.17% of customers with no previous default did not subscribe, while around 11.83% of them subscribed.\n",
    "\n",
    "- For customers with a previous default (default=yes):\n",
    "  - Count: 544 customers did not subscribe ('no'), and 41 customers subscribed ('yes').\n",
    "  - Percentage: Approximately 92.99% of customers with a previous default did not subscribe, while around 7.01% of them subscribed.\n",
    "\n",
    "This suggests that customers with no previous default are more likely to subscribe compared to those with a previous default, as indicated by the higher subscription rate (11.83% vs. 7.01%) among the former group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029face",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing vs Subscribed\n",
    "\n",
    "Housing = pd.crosstab(train['housing'], train['subscribed']) #Value Count\n",
    "Percentage = Housing.div(Housing.sum(1).astype(float), axis= 0) # Percentage\n",
    "\n",
    "housing_concat=pd.concat([Housing,Percentage], axis=1, keys=['Count','Percentage'])\n",
    "housing_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing it\n",
    "\n",
    "# First plot: Value count of housing vs Subscribed\n",
    "sns.countplot(x='housing', hue='subscribed', data=train)\n",
    "plt.title('Value count of Subscribed with Housing', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0584c79",
   "metadata": {},
   "source": [
    "### `Observation:`\n",
    "\n",
    "- **Count of Subscribed Customers:**\n",
    "  - For customers with \"housing\" status as \"no,\" there are 11,698 subscribers who have not subscribed (no), and 2,365 subscribers who have subscribed (yes).\n",
    "  - For customers with \"housing\" status as \"yes,\" there are 16,234 subscribers who have not subscribed (no), and 1,350 subscribers who have subscribed (yes).\n",
    "\n",
    "- **Percentage of Subscribed Customers:**\n",
    "  - Among customers with \"housing\" status as \"no,\" approximately 83.18% have not subscribed (no), while about 16.82% have subscribed (yes).\n",
    "  - Among customers with \"housing\" status as \"yes,\" approximately 92.32% have not subscribed (no), while about 7.68% have subscribed (yes).\n",
    "\n",
    "These observations provide insights into the relationship between housing status and subscription to a term deposit. It appears that customers with no housing tend to have a higher subscription rate compared to those with housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan vs subscribed\n",
    "\n",
    "loan_crosstab = pd.crosstab(train['loan'], train['subscribed']) #Value Count\n",
    "Percentage = loan_crosstab.div(loan_crosstab.sum(1).astype(float), axis= 0) # Percentage\n",
    "\n",
    "loan_concat=pd.concat([loan_crosstab,Percentage], axis=1, keys=['Count','Percentage'])\n",
    "loan_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing it\n",
    "\n",
    "# First plot: Value count of Loan vs Subscribed\n",
    "sns.countplot(x='loan', hue='subscribed', data=train)\n",
    "plt.title('Value count of Subscribed with Loan', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59020e46",
   "metadata": {},
   "source": [
    "### `Observation:`\n",
    "It is evident that the count and percentage of subscribers and non-subscribers categorized by whether or not they have a loan. The \"no\" category represents clients without a loan, while the \"yes\" category represents clients with a loan. It is evident that a higher percentage of clients without a loan subscribed to the term deposit compared to those with a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contact vs subscribed\n",
    "contact_crosstab = pd.crosstab(train['contact'], train['subscribed']) #Value Count\n",
    "Percentage = contact_crosstab.div(contact_crosstab.sum(1).astype(float), axis= 0) # Percentage\n",
    "\n",
    "contact_concat=pd.concat([contact_crosstab,Percentage], axis=1, keys=['Count','Percentage'])\n",
    "contact_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 4))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value count of Subscribed with Contact\n",
    "sns.countplot(x='contact', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"}, ax=axes)\n",
    "axes.set_title('Value count of Subscribed with Contact', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613675b",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "The count and percentage of subscribers and non-subscribers based on the contact method used. The \"cellular\" category represents clients contacted via cellular phones, \"telephone\" represents clients contacted via landline telephones, and \"unknown\" represents clients with unknown contact methods. It is evident that a higher percentage of subscribers were reached through cellular phones compared to other contact methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb78533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#month vs subscribed\n",
    "\n",
    "month_crosstab = pd.crosstab(train['month'], train['subscribed']) #Value Count\n",
    "Percentage = month_crosstab.div(month_crosstab.sum(1).astype(float), axis= 0) # Percentage\n",
    "\n",
    "month_concat=pd.concat([month_crosstab,Percentage], axis=1, keys=['Count','Percentage'])\n",
    "month_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 4))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value count of Subscribed with month\n",
    "sns.countplot(x='month', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"}, ax=axes)\n",
    "axes.set_title('Value count of Subscribed with month', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff0d8b",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "- In the month of April (apr), there were 1,671 subscriptions, with 384 (18.7%) being \"yes.\"\n",
    "\n",
    "- August (aug) had 3,813 subscriptions, with 520 (12%) of them being \"yes.\"\n",
    "\n",
    "- December (dec) had the smallest number of subscriptions, with 85 in total. However, a high percentage of them, 72 (45.9%), were \"yes.\"\n",
    "\n",
    "- February (feb) had 1,522 subscriptions, and 305 (16.7%) of them were \"yes.\"\n",
    "\n",
    "- January (jan) had 880 subscriptions, with 97 (9.9%) being \"yes.\"\n",
    "\n",
    "- July (jul) had 4,403 subscriptions, and 441 (9.1%) were \"yes.\"\n",
    "\n",
    "- June (jun) had 3,355 subscriptions, and 383 (10.2%) were \"yes.\"\n",
    "\n",
    "- March (mar) had 168 subscriptions, with 174 (50.9%) of them being \"yes.\"\n",
    "\n",
    "- May (may) had the highest number of subscriptions, with 9,020 in total. However, only 649 (6.7%) of them were \"yes.\"\n",
    "\n",
    "- November (nov) had 2,508 subscriptions, with 275 (9.9%) being \"yes.\"\n",
    "\n",
    "- October (oct) had 288 subscriptions, with 224 (43.8%) being \"yes.\"\n",
    "\n",
    "- September (sep) had 219 subscriptions, with 191 (46.6%) being \"yes.\"\n",
    "\n",
    "These observations provide insights into the distribution of subscriptions across different months, highlighting variations in both count and percentage of \"yes\" subscriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6894b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poutcome vs subscribed\n",
    "\n",
    "poutcome_crosstab = pd.crosstab(train['poutcome'], train['subscribed']) #Value Count\n",
    "Percentage = poutcome_crosstab.div(poutcome_crosstab.sum(1).astype(float), axis= 0) # Percentage\n",
    "\n",
    "poutcome_concat=pd.concat([poutcome_crosstab,Percentage], axis=1, keys=['Count','Percentage'])\n",
    "poutcome_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 4))  # Use plt.subplots to create a single subplot\n",
    "\n",
    "# Count plot: Value count of Subscribed with poutcome\n",
    "sns.countplot(x='poutcome', hue='subscribed', data=train, palette={\"yes\": \"g\", \"no\": \"r\"}, ax=axes)\n",
    "axes.set_title('Value count of Subscribed with poutcome', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f9f31",
   "metadata": {},
   "source": [
    "### ii. Bivariate Analysis for Numeric columns with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#campaign vs Subscribed\n",
    "train.groupby(['campaign'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f2901",
   "metadata": {},
   "source": [
    "### `Observation:`\n",
    "* The majority of clients were contacted fewer times during the campaign, with the most common counts being 1, 2, and 3.\n",
    "\n",
    "* As the number of campaign contacts increased beyond 3, the counts of clients contacted decreased significantly. This suggests that a large portion of clients were contacted relatively few times.\n",
    "\n",
    "* The subscription rate tends to decrease as the number of campaign contacts increases. Clients who were contacted only once or twice had a higher subscription rate compared to those contacted more frequently.\n",
    "\n",
    "* There is a noticeable drop in the subscription rate after the 6th campaign contact, with a steep decline in subscriptions as the number of contacts continues to increase.\n",
    "\n",
    "* It's interesting to note that there are instances where clients were contacted up to 63 times during the campaign, but the subscription rate remains low for these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1af3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Age vs Subscribed\n",
    "pd.set_option('display.max_rows', None)\n",
    "train.groupby(['age'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781579a3",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "* The age of 32 has the highest number of subscribers, with 1457 individuals.\n",
    "* Ages 31, 33, 34, and 35 also have a substantial number of subscribers, ranging from 1314 to 1417.\n",
    "* The number of subscribers generally decreases as age increases, with fewer subscribers in the older age groups.\n",
    "* There are very few subscribers in the age groups beyond 60, with ages 61 and above having the lowest numbers of subscribers.\n",
    "* Ages between 18 and 28 have a moderate number of subscribers.\n",
    "\n",
    "Overall, the data suggests that the majority of subscribers are in their early thirties, and the number of subscribers tends to decrease as age increases beyond that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e1d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Balance vs Subscribed\n",
    "train.groupby(['balance'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce83b70",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "It's evident that the balance variable doesn't have a significant impact on detecting whether someone has subscribed to the terms or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf16c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day vs Subscribed\n",
    "train.groupby(['day'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028b13e",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "1. The dataset contains two columns: \"day\" and \"subscribed.\"\n",
    "\n",
    "2. The \"day\" values range from 0 to 30, indicating a time period of one month.\n",
    "\n",
    "3. The \"subscribed\" values vary, with the highest being 1909 and the lowest being 220.\n",
    "\n",
    "4. There appears to be a general downward trend in the \"subscribed\" values as the \"day\" values increase, suggesting a potential decrease in subscriptions over time.\n",
    "\n",
    "5. There are a few instances where the \"subscribed\" values remain relatively stable despite changes in the \"day\" values, such as around days 16-17, 20-21, and 27-28.\n",
    "\n",
    "6. Day 19 has the highest number of subscriptions with 1909, indicating a potential peak in subscriptions during that day.\n",
    "\n",
    "7. Days 0 and 30 have the lowest number of subscriptions with 220 and 460, respectively, suggesting lower activity or interest in subscriptions at the beginning and end of the month.\n",
    "\n",
    "These observations provide an initial overview of the data and may serve as a basis for further analysis or exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pdays vs subscribed\n",
    "train.groupby(['pdays'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8ad0c",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "The timing of client contacts, as represented by \"pdays,\" can have an impact on subscription rates, with shorter time intervals since the last contact generally associated with higher subscription counts. However, this relationship is not strictly linear, and other factors may also play a role in determining subscription outcomes. Further analysis may be needed to explore these relationships in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration vs subscribed\n",
    "train.groupby(['duration'])['subscribed'].count().reset_index().sort_values(by='subscribed', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13528362",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "It suggest that contact duration isnot that imapact on detecting subscribe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulazing it\n",
    "plt.figure(figsize=(20,25))\n",
    "p=1\n",
    "for i in num_col:\n",
    "    if p<=8:\n",
    "        plt.subplot(4,2,p)\n",
    "        sns.distplot(train[train['subscribed'] == \"yes\"][i], label='Subscribed', color='g')\n",
    "        sns.distplot(train[train['subscribed'] == \"no\"][i], label='not subscribed', color='r')\n",
    "        plt.title(f'Distribution of {i} vs Subscribed')\n",
    "        plt.xlabel(i, fontsize=16)\n",
    "        plt.legend()\n",
    "    p+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa31cdb",
   "metadata": {},
   "source": [
    "## Outlier Analysis & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab85d",
   "metadata": {},
   "source": [
    "### a. TRAIN Data Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.boxplot(train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3bce9",
   "metadata": {},
   "source": [
    "### Handling Outliers using Zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(zscore(train[['age','balance','duration','campaign','pdays','previous']]))\n",
    "\n",
    "#dropping outliers\n",
    "train_new=train[(z<3).all(axis=1)]\n",
    "\n",
    "#Data loss percentage after using Zscore\n",
    "print(\"Old DataFrame: \",train.shape[0])\n",
    "print(\"New DataFrame: \",train_new.shape[0])\n",
    "data_loss_percentage= ((train.shape[0]-train_new.shape[0])/train.shape[0])*100\n",
    "print(\"Data loss Percentage: {:.2f}\".format(data_loss_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking with IQR method\n",
    "Q1=train.quantile(0.25)\n",
    "Q3=train.quantile(0.75)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "train2=train[~((train<(Q1-1.5 *IQR))|(train>(Q3+1.5*IQR))).any(axis=1)]\n",
    "print(\"Data Loss Percentage After removing Outliers with IQR method: \",((train.shape[0]-train2.shape[0])/train.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4204a3",
   "metadata": {},
   "source": [
    "### `Observation:`\n",
    "\n",
    "The Z-score method for outlier removal demonstrates a substantial reduction in data loss, with only 11.13% compared to the 37.44% observed with the IQR method. This pronounced disparity in data loss percentages strongly suggests that the Z-score method is the preferred approach for managing outliers in this dataset.\n",
    "\n",
    "Despite our efforts to address outliers, it is apparent that some still persist within our dataframe. Consequently, we will employ the Robust Scaler, known for its capability to effectively centers the data around the median and scales it by the spread of the middle 50% of the data, making it less sensitive to extreme values (outliers) compared to other scaling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c123922",
   "metadata": {},
   "source": [
    "### b. TEST Data Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.boxplot(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c173a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(zscore(test[['age','balance','duration','campaign','pdays','previous']]))\n",
    "\n",
    "#dropping outliers\n",
    "test_new=test[(z<3).all(axis=1)]\n",
    "\n",
    "#Data loss percentage after using Zscore\n",
    "print(\"Old DataFrame: \",test.shape[0])\n",
    "print(\"New DataFrame: \",test_new.shape[0])\n",
    "data_loss_percentage= ((test.shape[0]-test_new.shape[0])/test.shape[0])*100\n",
    "print(\"Data loss Percentage: {:.2f}\".format(data_loss_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1038e",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "The 'Data loss Percentage' after using the Z-score method for outlier removal is calculated to be approximately 11.28%. This percentage represents the proportion of data points that were identified as outliers and subsequently removed from the test dataset.\n",
    "\n",
    "It's important to note that we performed outlier removal in the test dataset following the same approach used for the train dataset. This consistency in data preprocessing ensures that our test data is prepared in a manner consistent with our training data, which is crucial for maintaining the integrity and reliability of our predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57adbbe7",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a459f",
   "metadata": {},
   "source": [
    "### a. TEST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a995cf",
   "metadata": {},
   "source": [
    "#### Converting Categorical column to numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644febfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for replacing 'yes' and 'no'\n",
    "yes_no_mapping = {'yes': 1, 'no': 0}\n",
    "\n",
    "# Replace values in the specified columns\n",
    "columns_to_replace = ['default', 'housing', 'loan', 'subscribed']\n",
    "train[columns_to_replace] = train[columns_to_replace].replace(yes_no_mapping)\n",
    "\n",
    "# Define a dictionary for replacing months\n",
    "month_mapping = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "    'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "    'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "\n",
    "# Replace month values\n",
    "train['month'] = train['month'].replace(month_mapping)\n",
    "\n",
    "# Replace 'unknown' job values with 'other'\n",
    "train['job'].replace('unknown', 'other', inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4efce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding other categorical columns using LabelEncoder\n",
    "le=LabelEncoder()\n",
    "train['job']=le.fit_transform(train['job'])\n",
    "train['education']=le.fit_transform(train['education'])\n",
    "train['poutcome']=le.fit_transform(train['poutcome'])\n",
    "train['marital']=le.fit_transform(train['marital'])\n",
    "train['contact']=le.fit_transform(train['contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193002",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "We have enconded all the categorical column into numerical column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45195ef9",
   "metadata": {},
   "source": [
    "### b. TEST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for replacing 'yes' and 'no'\n",
    "yes_no_mapping = {'yes': 1, 'no': 0}\n",
    "\n",
    "# Replace values in the specified columns\n",
    "columns_to_replace = ['default', 'housing', 'loan']\n",
    "test[columns_to_replace] = test[columns_to_replace].replace(yes_no_mapping)\n",
    "\n",
    "# Define a dictionary for replacing months\n",
    "month_mapping = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "    'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "    'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "# Replace month values\n",
    "test['month'] = test['month'].replace(month_mapping)\n",
    "\n",
    "# Replace 'unknown' job values with 'other'\n",
    "test['job'].replace('unknown', 'other', inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a03859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding other categorical columns using LabelEncoder\n",
    "le=LabelEncoder()\n",
    "test['job']=le.fit_transform(test['job'])\n",
    "test['education']=le.fit_transform(test['education'])\n",
    "test['poutcome']=le.fit_transform(test['poutcome'])\n",
    "test['marital']=le.fit_transform(test['marital'])\n",
    "test['contact']=le.fit_transform(test['contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b76f55",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "It is evident that we have successfully transformed categorical columns into numerical format for both the training and testing datasets using label encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6627c3e",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulaizing it\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(train.corr(), annot=True, fmt='.1F', cmap='coolwarm', annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da0691",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "Based on the correlation matrix, here are the observations:\n",
    "\n",
    "1. **Correlations with the Target Variable (subscribed):**\n",
    "   - The feature 'duration' has a relatively strong positive correlation with the target variable 'subscribed' (0.355888). This suggests that a longer duration of the last contact with the client is associated with a higher likelihood of subscription.\n",
    "   - 'pdays' and 'previous' also show moderate positive correlations with 'subscribed,' indicating that previous contacts and the number of days since the last contact have some influence on subscription.\n",
    "\n",
    "2. **Age and Marital Status:**\n",
    "   - 'age' has a negative correlation with 'marital' (-0.411087), suggesting that younger clients are more likely to be married.\n",
    "   - 'marital' and 'subscribed' have a positive correlation (0.056387), indicating that marital status may have some influence on subscription.\n",
    "\n",
    "3. **Contact Method:**\n",
    "   - 'contact' (contact method) has a relatively strong negative correlation with 'ID' (-0.734588) and 'default' (credit default), indicating that these features are somewhat related. This might suggest that certain contact methods are preferred or more effective for specific client profiles.\n",
    "\n",
    "4. **Previous Marketing Campaigns:**\n",
    "   - 'pdays' (number of days since the last contact) has strong positive correlations with 'previous' (number of previous contacts) and 'poutcome' (outcome of the previous marketing campaign). This is expected, as a longer time since the last contact would likely lead to more previous contacts and potentially different outcomes from previous campaigns.\n",
    "\n",
    "5. **Default, Housing, and Loan:**\n",
    "   - 'default' (credit default) has a positive correlation with 'balance' and 'previous,' but it doesn't show strong correlations with the other features or the target variable.\n",
    "   - 'housing' (housing loan) has a negative correlation with 'duration' and a positive correlation with 'contact.' This suggests some relationships with call duration and the preferred contact method among those with housing loans.\n",
    "   - 'loan' (personal loan) doesn't show strong correlations with other features or the target variable.\n",
    "\n",
    "6. **Month and Day:**\n",
    "   - 'month' and 'day' don't have strong correlations with most other features or the target variable, except for a moderate positive correlation between 'month' and 'poutcome' (0.040108). This might indicate some seasonality or temporal patterns in campaign outcomes.\n",
    "\n",
    "7. **Campaign-Related Features:**\n",
    "   - 'campaign' (number of contacts during this campaign) and 'poutcome' show some negative correlation (-0.086074), suggesting that more contacts in the current campaign may lead to a less favorable outcome if the previous campaign didn't succeed.\n",
    "\n",
    "8. **Balance:**\n",
    "   - 'balance' doesn't show strong correlations with most other features or the target variable, except for a weak positive correlation with 'duration' (0.031060).\n",
    "\n",
    "These observations provide insights into the relationships between different features in your dataset. They can be valuable for feature selection, understanding the data, and potentially building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffc679",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating feature & Label\n",
    "\n",
    "# Feature\n",
    "x = train.drop(columns=[\"ID\", \"subscribed\"])\n",
    "\n",
    "# Target\n",
    "y= train[\"subscribed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe040b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97838e5",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "We have effectively split the dataset into features and labels. Initially, our training dataset had dimensions (28124, 17). Now, with 'X' containing all the features, the shape of 'X' is (28124, 16), while 'y' contains our target variable, resulting in a shape of (28124,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Column from test Dataset\n",
    "\n",
    "test = test.drop(\"ID\", axis=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36909f2b",
   "metadata": {},
   "source": [
    "## Feature Scaling/Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafaba12",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the scaler on the features\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Create a DataFrame view of the scaled features after preprocessing\n",
    "scaled_df = pd.DataFrame(x_scaled, columns=x.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5bc08",
   "metadata": {},
   "source": [
    "### b. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198652d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the scaler on the features\n",
    "test_scaled = scaler.fit_transform(test)\n",
    "\n",
    "# Create a DataFrame view of the scaled features after preprocessing\n",
    "scaled_test = pd.DataFrame(test_scaled, columns=test.columns)\n",
    "scaled_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76c037",
   "metadata": {},
   "source": [
    "### `Observations`\n",
    "\n",
    "It's clear that we've effectively applied the Robust Scaler to standardize all the features of train dataset as well as test dataset, benefiting from its robustness to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af269c",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "# Previous vs New Class distribution\n",
    "print('Before Sampling:\\n', y.value_counts())\n",
    "print('\\nAfter Sampling:\\n', y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329801c",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "By applying sampling techniques, we have balanced the class distribution, ensuring that both classes now have an equal number of samples. This can help improve the performance and fairness of machine learning models, particularly in cases where class imbalance can impact model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8118b",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x_resampled, y_resampled, random_state=42, test_size= 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of x_train',x_train.shape)\n",
    "print('Shape of x_test',x_test.shape)\n",
    "print('Shape of y_train',y_train.shape)\n",
    "print('Shape of y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0c144",
   "metadata": {},
   "source": [
    "### `Observations:` \n",
    "\n",
    "The dataset has been successfully split into training and testing sets.This separation of data into training and testing subsets allows you to develop and evaluate machine learning models using the specified dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4119986",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd90c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Extra Tree': ExtraTreesClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661b141",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc762f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {'Classifier': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier and store results\n",
    "for clf_name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, x_resampled, y_resampled, cv=10, scoring='accuracy')\n",
    "\n",
    "    # Predict on cross validation\n",
    "    y_pred = cross_val_predict(clf, x_resampled, y_resampled, cv=5)\n",
    "\n",
    "    # Compute the metrics: Mean Accuracy, Precision, Recall, F1-Score\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    precision = precision_score(y_resampled, y_pred)\n",
    "    recall = recall_score(y_resampled, y_pred)\n",
    "    f1_score_value = f1_score(y_resampled, y_pred)\n",
    "\n",
    "\n",
    "    results['Classifier'].append(clf_name)\n",
    "    results['Accuracy'].append(mean_accuracy)\n",
    "    results['Precision'].append(precision)\n",
    "    results['Recall'].append(recall)\n",
    "    results['F1 Score'].append(f1_score_value)\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results as a DataFrame\n",
    "results_df.sort_values(by='F1 Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4849179",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "**Selection and Use of Models:**\n",
    "\n",
    "Based on the provided metrics, we would select the following three models:\n",
    "\n",
    "1. **Extra Tree:** This model has a high accuracy, precision, recall, and F1 score. It should be selected when a high overall performance is crucial, such as in applications where both false positives and false negatives are costly.\n",
    "\n",
    "2. **Random Forest:** Similar to Extra Tree, Random Forest also performs exceptionally well across all metrics. It is a robust choice for general classification tasks and can handle a variety of datasets effectively.\n",
    "\n",
    "These three models can be considered as top candidates for further evaluation or deployment, depending on the specific requirements and constraints of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6107a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Parameters for Exta Trees\n",
    "param_ET = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'criterion': ['gini', 'entropy','log_loss'],\n",
    "        'max_depth': [None, 5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "# Instantiate Parameters for Random Forest\n",
    "\n",
    "param_rf = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c15b78",
   "metadata": {},
   "source": [
    "### a. Hyper Parameter Tuning for Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb013680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate The Extra Trees Classifier object\n",
    "clf_ET = ExtraTreesClassifier()\n",
    "\n",
    "#Instantiate Randomized Search CV for Extra Trees Classifier\n",
    "random_search_ET = RandomizedSearchCV(clf_ET, param_distributions= param_ET,\n",
    "                                     n_iter=10, scoring='accuracy', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "#Fit the Data\n",
    "random_search_ET.fit(x_resampled, y_resampled)\n",
    "# Print the Best parameters and best score\n",
    "print(random_search_ET.best_params_)\n",
    "print(random_search_ET.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef59a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Extra Trees Classifier with hyperparameter tuning\n",
    "clf_ET = ExtraTreesClassifier(n_estimators=200, min_samples_split=10, max_depth=None, criterion='log_loss', random_state=42)\n",
    "\n",
    "# Fit the training data to the Extra Trees Classifier\n",
    "clf_ET.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the same classifier\n",
    "y_pred_ET = clf_ET.predict(x_test)\n",
    "\n",
    "# Print the classification report for the Extra Trees Classifier\n",
    "print(\"Extra Trees Classifier with Hyperparameter Tuning:\\n\")\n",
    "print(classification_report(y_test, y_pred_ET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b9f72",
   "metadata": {},
   "source": [
    "### b. Hyper Parameter Tuning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca68307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Random Forest Classifier Object\n",
    "clf_rf =  RandomForestClassifier()\n",
    "\n",
    "# Instantiate Randomized Search CV for Random Forest Classifier\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    clf_rf, param_distributions=param_rf,\n",
    "    n_iter=10, scoring='accuracy', cv=5, n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Data with hyperparameter tuning\n",
    "random_search_rf.fit(x_resampled, y_resampled)\n",
    "\n",
    "# Print the Best parameters and best score from hyperparameter tuning\n",
    "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
    "print(\"Best Score:\", random_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Random Forest Classifier with the best hyperparameters\n",
    "clf_rf = RandomForestClassifier(**random_search_rf.best_params_)\n",
    "\n",
    "# Fit the training data to the Random Forest Classifier\n",
    "clf_rf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the same classifier\n",
    "y_pred_rf = clf_rf.predict(x_test)\n",
    "\n",
    "# Print the classification report for the Random Forest Classifier with hyperparameter tuning\n",
    "print(\"\\nRandom Forest Classifier with Hyperparameter Tuning:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15a6b4",
   "metadata": {},
   "source": [
    "### Plotting Confusion Matrix\n",
    "* Extra Trees Classifier\n",
    "* Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e185fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrices for both models\n",
    "cm_ET = confusion_matrix(y_test, y_pred_ET)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Create subplots for the confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "\n",
    "# Plot Extra Trees Classifier Confusion Matrix\n",
    "sns.heatmap(cm_ET, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title('Extra Trees Classifier Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Labels')\n",
    "axes[0].set_ylabel('True Labels')\n",
    "\n",
    "# Plot Random Forest Confusion Matrix\n",
    "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title('Random Forest Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Labels')\n",
    "axes[1].set_ylabel('True Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e526a3",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "Both the Random Forest Classifier and the Extra Trees Classifier with hyperparameter tuning have very similar performance metrics, including accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296eea3",
   "metadata": {},
   "source": [
    "## ROC-AUC Curve:\n",
    "* Extra Trees Classifier\n",
    "* Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77763037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class\n",
    "y_prob = clf_ET.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC AUC score\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC Score Of Extra Tress Classifier:\", auc_score)\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_prob = clf_rf.predict_proba(x_test)[:, 1]\n",
    "# Calculate ROC AUC score\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC Score Of Random Forest Classifier:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class for both classifiers\n",
    "y_prob_et = clf_ET.predict_proba(x_test)[:, 1]\n",
    "y_prob_rf = clf_rf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC scores for both classifiers\n",
    "auc_score_et = roc_auc_score(y_test, y_prob_et)\n",
    "auc_score_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "# Calculate ROC curves for both classifiers\n",
    "fpr_et, tpr_et, _ = roc_curve(y_test, y_prob_et, pos_label=1)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf, pos_label=1)\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot ROC curve for Extra Trees Classifier\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_et, tpr_et, label='Extra Trees ROC Curve (AUC = {:.2f})'.format(auc_score_et))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Extra Trees')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot ROC curve for Random Forest Classifier\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest ROC Curve (AUC = {:.2f})'.format(auc_score_rf))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Adjust layout and show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91270fd8",
   "metadata": {},
   "source": [
    "### `Observations:`\n",
    "\n",
    "Both models have very high ROC AUC scores, suggesting that they are performing well in terms of distinguishing between the classes. However, the Random Forest Classifier has a slightly higher ROC AUC score, indicating slightly better performance in this specific metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b62074",
   "metadata": {},
   "source": [
    "## Predicting test data based on training data using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf_ET.predict(test)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5e68bbc",
   "metadata": {},
   "source": [
    "with open('best_model','wb') as f:\n",
    "    pickle.dump(clf_ET,f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d73a00c",
   "metadata": {},
   "source": [
    "pickle.dump(scaler, open('scaler.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
